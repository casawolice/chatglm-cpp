{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 42.25352112676056,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14084507042253522,
      "grad_norm": 4.10063362121582,
      "learning_rate": 9.966666666666667e-06,
      "loss": 4.2443,
      "step": 10
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 4.172516822814941,
      "learning_rate": 9.933333333333334e-06,
      "loss": 4.0879,
      "step": 20
    },
    {
      "epoch": 0.4225352112676056,
      "grad_norm": 5.482264995574951,
      "learning_rate": 9.9e-06,
      "loss": 3.9089,
      "step": 30
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 4.176920413970947,
      "learning_rate": 9.866666666666668e-06,
      "loss": 3.8756,
      "step": 40
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 5.125543594360352,
      "learning_rate": 9.833333333333333e-06,
      "loss": 3.8977,
      "step": 50
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 3.9578349590301514,
      "learning_rate": 9.800000000000001e-06,
      "loss": 3.9533,
      "step": 60
    },
    {
      "epoch": 0.9859154929577465,
      "grad_norm": 5.708184719085693,
      "learning_rate": 9.766666666666667e-06,
      "loss": 3.8816,
      "step": 70
    },
    {
      "epoch": 1.1267605633802817,
      "grad_norm": 6.460731029510498,
      "learning_rate": 9.733333333333334e-06,
      "loss": 4.1916,
      "step": 80
    },
    {
      "epoch": 1.267605633802817,
      "grad_norm": 5.894594669342041,
      "learning_rate": 9.7e-06,
      "loss": 3.8025,
      "step": 90
    },
    {
      "epoch": 1.408450704225352,
      "grad_norm": 8.054149627685547,
      "learning_rate": 9.666666666666667e-06,
      "loss": 3.4345,
      "step": 100
    },
    {
      "epoch": 1.5492957746478875,
      "grad_norm": 5.810593605041504,
      "learning_rate": 9.633333333333335e-06,
      "loss": 3.6396,
      "step": 110
    },
    {
      "epoch": 1.6901408450704225,
      "grad_norm": 7.092684745788574,
      "learning_rate": 9.600000000000001e-06,
      "loss": 3.3771,
      "step": 120
    },
    {
      "epoch": 1.8309859154929577,
      "grad_norm": 8.76417064666748,
      "learning_rate": 9.566666666666668e-06,
      "loss": 3.8227,
      "step": 130
    },
    {
      "epoch": 1.971830985915493,
      "grad_norm": 8.020915031433105,
      "learning_rate": 9.533333333333334e-06,
      "loss": 3.8066,
      "step": 140
    },
    {
      "epoch": 2.112676056338028,
      "grad_norm": 6.895535945892334,
      "learning_rate": 9.5e-06,
      "loss": 3.6266,
      "step": 150
    },
    {
      "epoch": 2.2535211267605635,
      "grad_norm": 6.443556785583496,
      "learning_rate": 9.466666666666667e-06,
      "loss": 3.2994,
      "step": 160
    },
    {
      "epoch": 2.3943661971830985,
      "grad_norm": 7.543540954589844,
      "learning_rate": 9.433333333333335e-06,
      "loss": 3.319,
      "step": 170
    },
    {
      "epoch": 2.535211267605634,
      "grad_norm": 6.320714473724365,
      "learning_rate": 9.4e-06,
      "loss": 3.0506,
      "step": 180
    },
    {
      "epoch": 2.676056338028169,
      "grad_norm": 9.178311347961426,
      "learning_rate": 9.366666666666668e-06,
      "loss": 3.0826,
      "step": 190
    },
    {
      "epoch": 2.816901408450704,
      "grad_norm": 9.015578269958496,
      "learning_rate": 9.333333333333334e-06,
      "loss": 2.916,
      "step": 200
    },
    {
      "epoch": 2.9577464788732395,
      "grad_norm": 8.527877807617188,
      "learning_rate": 9.3e-06,
      "loss": 2.9338,
      "step": 210
    },
    {
      "epoch": 3.0985915492957745,
      "grad_norm": 6.329701900482178,
      "learning_rate": 9.266666666666667e-06,
      "loss": 2.7638,
      "step": 220
    },
    {
      "epoch": 3.23943661971831,
      "grad_norm": 14.814711570739746,
      "learning_rate": 9.233333333333334e-06,
      "loss": 2.7158,
      "step": 230
    },
    {
      "epoch": 3.380281690140845,
      "grad_norm": 9.48200511932373,
      "learning_rate": 9.200000000000002e-06,
      "loss": 2.8822,
      "step": 240
    },
    {
      "epoch": 3.52112676056338,
      "grad_norm": 7.93746280670166,
      "learning_rate": 9.166666666666666e-06,
      "loss": 2.8146,
      "step": 250
    },
    {
      "epoch": 3.6619718309859155,
      "grad_norm": 9.189133644104004,
      "learning_rate": 9.133333333333335e-06,
      "loss": 2.6519,
      "step": 260
    },
    {
      "epoch": 3.802816901408451,
      "grad_norm": 5.8495378494262695,
      "learning_rate": 9.100000000000001e-06,
      "loss": 2.5355,
      "step": 270
    },
    {
      "epoch": 3.943661971830986,
      "grad_norm": 9.409466743469238,
      "learning_rate": 9.066666666666667e-06,
      "loss": 2.4424,
      "step": 280
    },
    {
      "epoch": 4.084507042253521,
      "grad_norm": 4.00587272644043,
      "learning_rate": 9.033333333333334e-06,
      "loss": 2.4442,
      "step": 290
    },
    {
      "epoch": 4.225352112676056,
      "grad_norm": 8.079556465148926,
      "learning_rate": 9e-06,
      "loss": 2.7018,
      "step": 300
    },
    {
      "epoch": 4.366197183098592,
      "grad_norm": 9.2967529296875,
      "learning_rate": 8.966666666666667e-06,
      "loss": 2.458,
      "step": 310
    },
    {
      "epoch": 4.507042253521127,
      "grad_norm": 10.034417152404785,
      "learning_rate": 8.933333333333333e-06,
      "loss": 2.2695,
      "step": 320
    },
    {
      "epoch": 4.647887323943662,
      "grad_norm": 8.21282958984375,
      "learning_rate": 8.900000000000001e-06,
      "loss": 2.0823,
      "step": 330
    },
    {
      "epoch": 4.788732394366197,
      "grad_norm": 9.844461441040039,
      "learning_rate": 8.866666666666668e-06,
      "loss": 2.0637,
      "step": 340
    },
    {
      "epoch": 4.929577464788732,
      "grad_norm": 9.984902381896973,
      "learning_rate": 8.833333333333334e-06,
      "loss": 2.2369,
      "step": 350
    },
    {
      "epoch": 5.070422535211268,
      "grad_norm": 7.846179485321045,
      "learning_rate": 8.8e-06,
      "loss": 2.1499,
      "step": 360
    },
    {
      "epoch": 5.211267605633803,
      "grad_norm": 4.023232936859131,
      "learning_rate": 8.766666666666669e-06,
      "loss": 2.0701,
      "step": 370
    },
    {
      "epoch": 5.352112676056338,
      "grad_norm": 9.058720588684082,
      "learning_rate": 8.733333333333333e-06,
      "loss": 2.1179,
      "step": 380
    },
    {
      "epoch": 5.492957746478873,
      "grad_norm": 8.704998970031738,
      "learning_rate": 8.700000000000001e-06,
      "loss": 1.9363,
      "step": 390
    },
    {
      "epoch": 5.633802816901408,
      "grad_norm": 5.455039978027344,
      "learning_rate": 8.666666666666668e-06,
      "loss": 1.9433,
      "step": 400
    },
    {
      "epoch": 5.774647887323944,
      "grad_norm": 6.945196628570557,
      "learning_rate": 8.633333333333334e-06,
      "loss": 1.8368,
      "step": 410
    },
    {
      "epoch": 5.915492957746479,
      "grad_norm": 9.737011909484863,
      "learning_rate": 8.6e-06,
      "loss": 1.8122,
      "step": 420
    },
    {
      "epoch": 6.056338028169014,
      "grad_norm": 8.006975173950195,
      "learning_rate": 8.566666666666667e-06,
      "loss": 1.7764,
      "step": 430
    },
    {
      "epoch": 6.197183098591549,
      "grad_norm": 6.564894676208496,
      "learning_rate": 8.533333333333335e-06,
      "loss": 1.8771,
      "step": 440
    },
    {
      "epoch": 6.338028169014084,
      "grad_norm": 9.543867111206055,
      "learning_rate": 8.5e-06,
      "loss": 1.6626,
      "step": 450
    },
    {
      "epoch": 6.47887323943662,
      "grad_norm": 17.63461685180664,
      "learning_rate": 8.466666666666668e-06,
      "loss": 1.9581,
      "step": 460
    },
    {
      "epoch": 6.619718309859155,
      "grad_norm": 6.655249118804932,
      "learning_rate": 8.433333333333334e-06,
      "loss": 1.5646,
      "step": 470
    },
    {
      "epoch": 6.76056338028169,
      "grad_norm": 6.011415958404541,
      "learning_rate": 8.400000000000001e-06,
      "loss": 1.5365,
      "step": 480
    },
    {
      "epoch": 6.901408450704225,
      "grad_norm": 7.984725475311279,
      "learning_rate": 8.366666666666667e-06,
      "loss": 1.787,
      "step": 490
    },
    {
      "epoch": 7.042253521126761,
      "grad_norm": 6.240879058837891,
      "learning_rate": 8.333333333333334e-06,
      "loss": 1.4931,
      "step": 500
    },
    {
      "epoch": 7.042253521126761,
      "eval_bleu-4": 0.0616335699261594,
      "eval_rouge-1": 31.882592,
      "eval_rouge-2": 11.000397999999997,
      "eval_rouge-l": 26.197778000000003,
      "eval_runtime": 76.1824,
      "eval_samples_per_second": 0.656,
      "eval_steps_per_second": 0.053,
      "step": 500
    },
    {
      "epoch": 7.183098591549296,
      "grad_norm": 6.618438243865967,
      "learning_rate": 8.3e-06,
      "loss": 1.6478,
      "step": 510
    },
    {
      "epoch": 7.323943661971831,
      "grad_norm": 7.621838569641113,
      "learning_rate": 8.266666666666667e-06,
      "loss": 1.5548,
      "step": 520
    },
    {
      "epoch": 7.464788732394366,
      "grad_norm": 9.24238395690918,
      "learning_rate": 8.233333333333335e-06,
      "loss": 1.5878,
      "step": 530
    },
    {
      "epoch": 7.605633802816901,
      "grad_norm": 16.623207092285156,
      "learning_rate": 8.2e-06,
      "loss": 1.7763,
      "step": 540
    },
    {
      "epoch": 7.746478873239437,
      "grad_norm": 6.83052396774292,
      "learning_rate": 8.166666666666668e-06,
      "loss": 1.4887,
      "step": 550
    },
    {
      "epoch": 7.887323943661972,
      "grad_norm": 10.65246295928955,
      "learning_rate": 8.133333333333334e-06,
      "loss": 1.4258,
      "step": 560
    },
    {
      "epoch": 8.028169014084508,
      "grad_norm": 8.887125968933105,
      "learning_rate": 8.1e-06,
      "loss": 1.2756,
      "step": 570
    },
    {
      "epoch": 8.169014084507042,
      "grad_norm": 7.623234272003174,
      "learning_rate": 8.066666666666667e-06,
      "loss": 1.4705,
      "step": 580
    },
    {
      "epoch": 8.309859154929578,
      "grad_norm": 9.871253967285156,
      "learning_rate": 8.033333333333335e-06,
      "loss": 1.4791,
      "step": 590
    },
    {
      "epoch": 8.450704225352112,
      "grad_norm": 8.173694610595703,
      "learning_rate": 8.000000000000001e-06,
      "loss": 1.2836,
      "step": 600
    },
    {
      "epoch": 8.591549295774648,
      "grad_norm": 10.199281692504883,
      "learning_rate": 7.966666666666668e-06,
      "loss": 1.5819,
      "step": 610
    },
    {
      "epoch": 8.732394366197184,
      "grad_norm": 10.737335205078125,
      "learning_rate": 7.933333333333334e-06,
      "loss": 1.5971,
      "step": 620
    },
    {
      "epoch": 8.873239436619718,
      "grad_norm": 6.5632643699646,
      "learning_rate": 7.9e-06,
      "loss": 1.2462,
      "step": 630
    },
    {
      "epoch": 9.014084507042254,
      "grad_norm": 11.214744567871094,
      "learning_rate": 7.866666666666667e-06,
      "loss": 1.1352,
      "step": 640
    },
    {
      "epoch": 9.154929577464788,
      "grad_norm": 9.323336601257324,
      "learning_rate": 7.833333333333333e-06,
      "loss": 1.4833,
      "step": 650
    },
    {
      "epoch": 9.295774647887324,
      "grad_norm": 7.45640754699707,
      "learning_rate": 7.800000000000002e-06,
      "loss": 1.2677,
      "step": 660
    },
    {
      "epoch": 9.43661971830986,
      "grad_norm": 11.134074211120605,
      "learning_rate": 7.766666666666666e-06,
      "loss": 1.278,
      "step": 670
    },
    {
      "epoch": 9.577464788732394,
      "grad_norm": 9.586923599243164,
      "learning_rate": 7.733333333333334e-06,
      "loss": 1.1555,
      "step": 680
    },
    {
      "epoch": 9.71830985915493,
      "grad_norm": 11.033470153808594,
      "learning_rate": 7.7e-06,
      "loss": 1.0413,
      "step": 690
    },
    {
      "epoch": 9.859154929577464,
      "grad_norm": 10.685467720031738,
      "learning_rate": 7.666666666666667e-06,
      "loss": 1.4286,
      "step": 700
    },
    {
      "epoch": 10.0,
      "grad_norm": 10.761895179748535,
      "learning_rate": 7.633333333333334e-06,
      "loss": 1.2865,
      "step": 710
    },
    {
      "epoch": 10.140845070422536,
      "grad_norm": 9.402318954467773,
      "learning_rate": 7.600000000000001e-06,
      "loss": 1.1333,
      "step": 720
    },
    {
      "epoch": 10.28169014084507,
      "grad_norm": 8.766775131225586,
      "learning_rate": 7.566666666666667e-06,
      "loss": 1.1278,
      "step": 730
    },
    {
      "epoch": 10.422535211267606,
      "grad_norm": 10.88211727142334,
      "learning_rate": 7.533333333333334e-06,
      "loss": 1.3303,
      "step": 740
    },
    {
      "epoch": 10.56338028169014,
      "grad_norm": 11.610977172851562,
      "learning_rate": 7.500000000000001e-06,
      "loss": 1.1589,
      "step": 750
    },
    {
      "epoch": 10.704225352112676,
      "grad_norm": 6.542832374572754,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 1.1627,
      "step": 760
    },
    {
      "epoch": 10.845070422535212,
      "grad_norm": 8.818622589111328,
      "learning_rate": 7.433333333333334e-06,
      "loss": 1.2036,
      "step": 770
    },
    {
      "epoch": 10.985915492957746,
      "grad_norm": 7.646493434906006,
      "learning_rate": 7.4e-06,
      "loss": 1.0547,
      "step": 780
    },
    {
      "epoch": 11.126760563380282,
      "grad_norm": 8.854241371154785,
      "learning_rate": 7.3666666666666676e-06,
      "loss": 0.9922,
      "step": 790
    },
    {
      "epoch": 11.267605633802816,
      "grad_norm": 9.472576141357422,
      "learning_rate": 7.333333333333333e-06,
      "loss": 0.979,
      "step": 800
    },
    {
      "epoch": 11.408450704225352,
      "grad_norm": 8.68674373626709,
      "learning_rate": 7.3e-06,
      "loss": 1.0142,
      "step": 810
    },
    {
      "epoch": 11.549295774647888,
      "grad_norm": 8.037264823913574,
      "learning_rate": 7.266666666666668e-06,
      "loss": 1.2424,
      "step": 820
    },
    {
      "epoch": 11.690140845070422,
      "grad_norm": 13.716623306274414,
      "learning_rate": 7.233333333333334e-06,
      "loss": 1.3124,
      "step": 830
    },
    {
      "epoch": 11.830985915492958,
      "grad_norm": 7.1950297355651855,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 1.0147,
      "step": 840
    },
    {
      "epoch": 11.971830985915492,
      "grad_norm": 9.038818359375,
      "learning_rate": 7.166666666666667e-06,
      "loss": 1.1144,
      "step": 850
    },
    {
      "epoch": 12.112676056338028,
      "grad_norm": 11.912760734558105,
      "learning_rate": 7.133333333333334e-06,
      "loss": 0.9912,
      "step": 860
    },
    {
      "epoch": 12.253521126760564,
      "grad_norm": 9.92718505859375,
      "learning_rate": 7.100000000000001e-06,
      "loss": 0.9561,
      "step": 870
    },
    {
      "epoch": 12.394366197183098,
      "grad_norm": 8.080711364746094,
      "learning_rate": 7.066666666666667e-06,
      "loss": 1.2492,
      "step": 880
    },
    {
      "epoch": 12.535211267605634,
      "grad_norm": 6.145348072052002,
      "learning_rate": 7.033333333333334e-06,
      "loss": 0.9305,
      "step": 890
    },
    {
      "epoch": 12.676056338028168,
      "grad_norm": 6.094018459320068,
      "learning_rate": 7e-06,
      "loss": 1.0354,
      "step": 900
    },
    {
      "epoch": 12.816901408450704,
      "grad_norm": 10.896116256713867,
      "learning_rate": 6.966666666666667e-06,
      "loss": 0.9564,
      "step": 910
    },
    {
      "epoch": 12.95774647887324,
      "grad_norm": 7.025166034698486,
      "learning_rate": 6.9333333333333344e-06,
      "loss": 1.053,
      "step": 920
    },
    {
      "epoch": 13.098591549295774,
      "grad_norm": 10.858709335327148,
      "learning_rate": 6.9e-06,
      "loss": 1.1664,
      "step": 930
    },
    {
      "epoch": 13.23943661971831,
      "grad_norm": 9.027238845825195,
      "learning_rate": 6.866666666666667e-06,
      "loss": 0.8497,
      "step": 940
    },
    {
      "epoch": 13.380281690140846,
      "grad_norm": 10.577934265136719,
      "learning_rate": 6.833333333333334e-06,
      "loss": 0.9103,
      "step": 950
    },
    {
      "epoch": 13.52112676056338,
      "grad_norm": 9.475231170654297,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.6944,
      "step": 960
    },
    {
      "epoch": 13.661971830985916,
      "grad_norm": 10.789628982543945,
      "learning_rate": 6.7666666666666665e-06,
      "loss": 0.7997,
      "step": 970
    },
    {
      "epoch": 13.80281690140845,
      "grad_norm": 11.781651496887207,
      "learning_rate": 6.733333333333334e-06,
      "loss": 1.07,
      "step": 980
    },
    {
      "epoch": 13.943661971830986,
      "grad_norm": 15.47423267364502,
      "learning_rate": 6.700000000000001e-06,
      "loss": 1.1367,
      "step": 990
    },
    {
      "epoch": 14.084507042253522,
      "grad_norm": 6.195760726928711,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.8533,
      "step": 1000
    },
    {
      "epoch": 14.084507042253522,
      "eval_bleu-4": 0.23954522889966462,
      "eval_rouge-1": 56.22518,
      "eval_rouge-2": 28.801519999999996,
      "eval_rouge-l": 45.07186000000001,
      "eval_runtime": 41.4326,
      "eval_samples_per_second": 1.207,
      "eval_steps_per_second": 0.097,
      "step": 1000
    },
    {
      "epoch": 14.225352112676056,
      "grad_norm": 13.802841186523438,
      "learning_rate": 6.633333333333334e-06,
      "loss": 1.047,
      "step": 1010
    },
    {
      "epoch": 14.366197183098592,
      "grad_norm": 7.860304832458496,
      "learning_rate": 6.600000000000001e-06,
      "loss": 0.9529,
      "step": 1020
    },
    {
      "epoch": 14.507042253521126,
      "grad_norm": 10.430211067199707,
      "learning_rate": 6.566666666666667e-06,
      "loss": 0.8701,
      "step": 1030
    },
    {
      "epoch": 14.647887323943662,
      "grad_norm": 11.973184585571289,
      "learning_rate": 6.533333333333334e-06,
      "loss": 0.9764,
      "step": 1040
    },
    {
      "epoch": 14.788732394366198,
      "grad_norm": 10.551779747009277,
      "learning_rate": 6.5000000000000004e-06,
      "loss": 0.9403,
      "step": 1050
    },
    {
      "epoch": 14.929577464788732,
      "grad_norm": 14.340288162231445,
      "learning_rate": 6.466666666666667e-06,
      "loss": 0.8125,
      "step": 1060
    },
    {
      "epoch": 15.070422535211268,
      "grad_norm": 10.172248840332031,
      "learning_rate": 6.433333333333333e-06,
      "loss": 0.769,
      "step": 1070
    },
    {
      "epoch": 15.211267605633802,
      "grad_norm": 10.488499641418457,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.9142,
      "step": 1080
    },
    {
      "epoch": 15.352112676056338,
      "grad_norm": 11.491652488708496,
      "learning_rate": 6.366666666666668e-06,
      "loss": 0.8504,
      "step": 1090
    },
    {
      "epoch": 15.492957746478874,
      "grad_norm": 13.503780364990234,
      "learning_rate": 6.333333333333333e-06,
      "loss": 0.9317,
      "step": 1100
    },
    {
      "epoch": 15.633802816901408,
      "grad_norm": 16.82074546813965,
      "learning_rate": 6.300000000000001e-06,
      "loss": 0.7819,
      "step": 1110
    },
    {
      "epoch": 15.774647887323944,
      "grad_norm": 10.815293312072754,
      "learning_rate": 6.266666666666668e-06,
      "loss": 0.7136,
      "step": 1120
    },
    {
      "epoch": 15.915492957746478,
      "grad_norm": 10.038507461547852,
      "learning_rate": 6.2333333333333335e-06,
      "loss": 1.0012,
      "step": 1130
    },
    {
      "epoch": 16.056338028169016,
      "grad_norm": 7.722906112670898,
      "learning_rate": 6.200000000000001e-06,
      "loss": 0.7452,
      "step": 1140
    },
    {
      "epoch": 16.197183098591548,
      "grad_norm": 9.958971977233887,
      "learning_rate": 6.166666666666667e-06,
      "loss": 0.7195,
      "step": 1150
    },
    {
      "epoch": 16.338028169014084,
      "grad_norm": 26.242582321166992,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.8892,
      "step": 1160
    },
    {
      "epoch": 16.47887323943662,
      "grad_norm": 11.917793273925781,
      "learning_rate": 6.1e-06,
      "loss": 0.6497,
      "step": 1170
    },
    {
      "epoch": 16.619718309859156,
      "grad_norm": 10.567105293273926,
      "learning_rate": 6.066666666666667e-06,
      "loss": 1.0866,
      "step": 1180
    },
    {
      "epoch": 16.760563380281692,
      "grad_norm": 15.315237998962402,
      "learning_rate": 6.033333333333335e-06,
      "loss": 0.5839,
      "step": 1190
    },
    {
      "epoch": 16.901408450704224,
      "grad_norm": 10.929169654846191,
      "learning_rate": 6e-06,
      "loss": 0.8673,
      "step": 1200
    },
    {
      "epoch": 17.04225352112676,
      "grad_norm": 11.766796112060547,
      "learning_rate": 5.966666666666667e-06,
      "loss": 0.7945,
      "step": 1210
    },
    {
      "epoch": 17.183098591549296,
      "grad_norm": 11.127120018005371,
      "learning_rate": 5.933333333333335e-06,
      "loss": 0.8958,
      "step": 1220
    },
    {
      "epoch": 17.323943661971832,
      "grad_norm": 9.283790588378906,
      "learning_rate": 5.9e-06,
      "loss": 0.8206,
      "step": 1230
    },
    {
      "epoch": 17.464788732394368,
      "grad_norm": 14.120203971862793,
      "learning_rate": 5.8666666666666675e-06,
      "loss": 0.6862,
      "step": 1240
    },
    {
      "epoch": 17.6056338028169,
      "grad_norm": 10.684391021728516,
      "learning_rate": 5.833333333333334e-06,
      "loss": 0.7169,
      "step": 1250
    },
    {
      "epoch": 17.746478873239436,
      "grad_norm": 13.035136222839355,
      "learning_rate": 5.8e-06,
      "loss": 0.5262,
      "step": 1260
    },
    {
      "epoch": 17.887323943661972,
      "grad_norm": 17.98885726928711,
      "learning_rate": 5.766666666666667e-06,
      "loss": 0.7258,
      "step": 1270
    },
    {
      "epoch": 18.028169014084508,
      "grad_norm": 11.798625946044922,
      "learning_rate": 5.733333333333334e-06,
      "loss": 0.9244,
      "step": 1280
    },
    {
      "epoch": 18.169014084507044,
      "grad_norm": 8.810370445251465,
      "learning_rate": 5.7e-06,
      "loss": 0.488,
      "step": 1290
    },
    {
      "epoch": 18.309859154929576,
      "grad_norm": 13.078669548034668,
      "learning_rate": 5.666666666666667e-06,
      "loss": 0.6811,
      "step": 1300
    },
    {
      "epoch": 18.450704225352112,
      "grad_norm": 13.022236824035645,
      "learning_rate": 5.633333333333334e-06,
      "loss": 0.7268,
      "step": 1310
    },
    {
      "epoch": 18.591549295774648,
      "grad_norm": 13.740148544311523,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.7616,
      "step": 1320
    },
    {
      "epoch": 18.732394366197184,
      "grad_norm": 21.724409103393555,
      "learning_rate": 5.566666666666667e-06,
      "loss": 0.6763,
      "step": 1330
    },
    {
      "epoch": 18.87323943661972,
      "grad_norm": 13.950478553771973,
      "learning_rate": 5.533333333333334e-06,
      "loss": 0.7311,
      "step": 1340
    },
    {
      "epoch": 19.014084507042252,
      "grad_norm": 12.305893898010254,
      "learning_rate": 5.500000000000001e-06,
      "loss": 0.8338,
      "step": 1350
    },
    {
      "epoch": 19.154929577464788,
      "grad_norm": 11.157687187194824,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.7512,
      "step": 1360
    },
    {
      "epoch": 19.295774647887324,
      "grad_norm": 7.96285343170166,
      "learning_rate": 5.4333333333333335e-06,
      "loss": 0.5364,
      "step": 1370
    },
    {
      "epoch": 19.43661971830986,
      "grad_norm": 13.294232368469238,
      "learning_rate": 5.400000000000001e-06,
      "loss": 0.9367,
      "step": 1380
    },
    {
      "epoch": 19.577464788732396,
      "grad_norm": 14.588299751281738,
      "learning_rate": 5.366666666666666e-06,
      "loss": 0.7297,
      "step": 1390
    },
    {
      "epoch": 19.718309859154928,
      "grad_norm": 13.56319522857666,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.5498,
      "step": 1400
    },
    {
      "epoch": 19.859154929577464,
      "grad_norm": 14.200268745422363,
      "learning_rate": 5.300000000000001e-06,
      "loss": 0.7244,
      "step": 1410
    },
    {
      "epoch": 20.0,
      "grad_norm": 2.898186206817627,
      "learning_rate": 5.2666666666666665e-06,
      "loss": 0.4629,
      "step": 1420
    },
    {
      "epoch": 20.140845070422536,
      "grad_norm": 11.344435691833496,
      "learning_rate": 5.233333333333334e-06,
      "loss": 0.5421,
      "step": 1430
    },
    {
      "epoch": 20.281690140845072,
      "grad_norm": 15.068236351013184,
      "learning_rate": 5.2e-06,
      "loss": 0.7229,
      "step": 1440
    },
    {
      "epoch": 20.422535211267604,
      "grad_norm": 16.468002319335938,
      "learning_rate": 5.1666666666666675e-06,
      "loss": 0.8387,
      "step": 1450
    },
    {
      "epoch": 20.56338028169014,
      "grad_norm": 11.012260437011719,
      "learning_rate": 5.133333333333334e-06,
      "loss": 0.5946,
      "step": 1460
    },
    {
      "epoch": 20.704225352112676,
      "grad_norm": 7.783412933349609,
      "learning_rate": 5.1e-06,
      "loss": 0.4949,
      "step": 1470
    },
    {
      "epoch": 20.845070422535212,
      "grad_norm": 13.982738494873047,
      "learning_rate": 5.0666666666666676e-06,
      "loss": 0.5791,
      "step": 1480
    },
    {
      "epoch": 20.985915492957748,
      "grad_norm": 14.028536796569824,
      "learning_rate": 5.033333333333333e-06,
      "loss": 0.6712,
      "step": 1490
    },
    {
      "epoch": 21.12676056338028,
      "grad_norm": 6.859469890594482,
      "learning_rate": 5e-06,
      "loss": 0.5856,
      "step": 1500
    },
    {
      "epoch": 21.12676056338028,
      "eval_bleu-4": 0.20629630179918543,
      "eval_rouge-1": 55.42497,
      "eval_rouge-2": 25.11958,
      "eval_rouge-l": 44.326319999999996,
      "eval_runtime": 52.6601,
      "eval_samples_per_second": 0.949,
      "eval_steps_per_second": 0.076,
      "step": 1500
    },
    {
      "epoch": 21.267605633802816,
      "grad_norm": 11.652098655700684,
      "learning_rate": 4.966666666666667e-06,
      "loss": 0.5029,
      "step": 1510
    },
    {
      "epoch": 21.408450704225352,
      "grad_norm": 14.227142333984375,
      "learning_rate": 4.933333333333334e-06,
      "loss": 0.6035,
      "step": 1520
    },
    {
      "epoch": 21.549295774647888,
      "grad_norm": 17.253324508666992,
      "learning_rate": 4.9000000000000005e-06,
      "loss": 0.6744,
      "step": 1530
    },
    {
      "epoch": 21.690140845070424,
      "grad_norm": 26.865360260009766,
      "learning_rate": 4.866666666666667e-06,
      "loss": 0.8685,
      "step": 1540
    },
    {
      "epoch": 21.830985915492956,
      "grad_norm": 15.425976753234863,
      "learning_rate": 4.833333333333333e-06,
      "loss": 0.2831,
      "step": 1550
    },
    {
      "epoch": 21.971830985915492,
      "grad_norm": 14.757665634155273,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.661,
      "step": 1560
    },
    {
      "epoch": 22.112676056338028,
      "grad_norm": 13.333877563476562,
      "learning_rate": 4.766666666666667e-06,
      "loss": 0.5147,
      "step": 1570
    },
    {
      "epoch": 22.253521126760564,
      "grad_norm": 13.925893783569336,
      "learning_rate": 4.7333333333333335e-06,
      "loss": 0.3705,
      "step": 1580
    },
    {
      "epoch": 22.3943661971831,
      "grad_norm": 12.498260498046875,
      "learning_rate": 4.7e-06,
      "loss": 0.4781,
      "step": 1590
    },
    {
      "epoch": 22.535211267605632,
      "grad_norm": 12.309127807617188,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.4749,
      "step": 1600
    },
    {
      "epoch": 22.676056338028168,
      "grad_norm": 14.992826461791992,
      "learning_rate": 4.633333333333334e-06,
      "loss": 0.5637,
      "step": 1610
    },
    {
      "epoch": 22.816901408450704,
      "grad_norm": 13.190740585327148,
      "learning_rate": 4.600000000000001e-06,
      "loss": 1.0006,
      "step": 1620
    },
    {
      "epoch": 22.95774647887324,
      "grad_norm": 15.858078002929688,
      "learning_rate": 4.566666666666667e-06,
      "loss": 0.5487,
      "step": 1630
    },
    {
      "epoch": 23.098591549295776,
      "grad_norm": 15.867474555969238,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.5461,
      "step": 1640
    },
    {
      "epoch": 23.239436619718308,
      "grad_norm": 6.857738018035889,
      "learning_rate": 4.5e-06,
      "loss": 0.4671,
      "step": 1650
    },
    {
      "epoch": 23.380281690140844,
      "grad_norm": 1.262386679649353,
      "learning_rate": 4.4666666666666665e-06,
      "loss": 0.4825,
      "step": 1660
    },
    {
      "epoch": 23.52112676056338,
      "grad_norm": 18.455503463745117,
      "learning_rate": 4.433333333333334e-06,
      "loss": 0.5904,
      "step": 1670
    },
    {
      "epoch": 23.661971830985916,
      "grad_norm": 32.15092849731445,
      "learning_rate": 4.4e-06,
      "loss": 0.5534,
      "step": 1680
    },
    {
      "epoch": 23.802816901408452,
      "grad_norm": 22.386571884155273,
      "learning_rate": 4.366666666666667e-06,
      "loss": 0.576,
      "step": 1690
    },
    {
      "epoch": 23.943661971830984,
      "grad_norm": 9.756600379943848,
      "learning_rate": 4.333333333333334e-06,
      "loss": 0.406,
      "step": 1700
    },
    {
      "epoch": 24.08450704225352,
      "grad_norm": 7.673166275024414,
      "learning_rate": 4.3e-06,
      "loss": 0.4578,
      "step": 1710
    },
    {
      "epoch": 24.225352112676056,
      "grad_norm": 12.195568084716797,
      "learning_rate": 4.266666666666668e-06,
      "loss": 0.5203,
      "step": 1720
    },
    {
      "epoch": 24.366197183098592,
      "grad_norm": 2.055670738220215,
      "learning_rate": 4.233333333333334e-06,
      "loss": 0.4826,
      "step": 1730
    },
    {
      "epoch": 24.507042253521128,
      "grad_norm": 19.63995933532715,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 0.6922,
      "step": 1740
    },
    {
      "epoch": 24.647887323943664,
      "grad_norm": 14.77818489074707,
      "learning_rate": 4.166666666666667e-06,
      "loss": 0.4871,
      "step": 1750
    },
    {
      "epoch": 24.788732394366196,
      "grad_norm": 15.630837440490723,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.4423,
      "step": 1760
    },
    {
      "epoch": 24.929577464788732,
      "grad_norm": 33.27202224731445,
      "learning_rate": 4.1e-06,
      "loss": 0.5271,
      "step": 1770
    },
    {
      "epoch": 25.070422535211268,
      "grad_norm": 1.0404669046401978,
      "learning_rate": 4.066666666666667e-06,
      "loss": 0.2729,
      "step": 1780
    },
    {
      "epoch": 25.211267605633804,
      "grad_norm": 20.54457664489746,
      "learning_rate": 4.033333333333333e-06,
      "loss": 0.4683,
      "step": 1790
    },
    {
      "epoch": 25.352112676056336,
      "grad_norm": 12.887994766235352,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.5513,
      "step": 1800
    },
    {
      "epoch": 25.492957746478872,
      "grad_norm": 14.75023365020752,
      "learning_rate": 3.966666666666667e-06,
      "loss": 0.8135,
      "step": 1810
    },
    {
      "epoch": 25.633802816901408,
      "grad_norm": 10.451302528381348,
      "learning_rate": 3.9333333333333335e-06,
      "loss": 0.2948,
      "step": 1820
    },
    {
      "epoch": 25.774647887323944,
      "grad_norm": 14.589095115661621,
      "learning_rate": 3.900000000000001e-06,
      "loss": 0.4809,
      "step": 1830
    },
    {
      "epoch": 25.91549295774648,
      "grad_norm": 9.833515167236328,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.3698,
      "step": 1840
    },
    {
      "epoch": 26.056338028169016,
      "grad_norm": 13.572084426879883,
      "learning_rate": 3.833333333333334e-06,
      "loss": 0.3989,
      "step": 1850
    },
    {
      "epoch": 26.197183098591548,
      "grad_norm": 4.859242916107178,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 0.5016,
      "step": 1860
    },
    {
      "epoch": 26.338028169014084,
      "grad_norm": 9.724010467529297,
      "learning_rate": 3.766666666666667e-06,
      "loss": 0.3336,
      "step": 1870
    },
    {
      "epoch": 26.47887323943662,
      "grad_norm": 14.589303970336914,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.4883,
      "step": 1880
    },
    {
      "epoch": 26.619718309859156,
      "grad_norm": 32.88545608520508,
      "learning_rate": 3.7e-06,
      "loss": 0.4106,
      "step": 1890
    },
    {
      "epoch": 26.760563380281692,
      "grad_norm": 16.9293270111084,
      "learning_rate": 3.6666666666666666e-06,
      "loss": 0.4969,
      "step": 1900
    },
    {
      "epoch": 26.901408450704224,
      "grad_norm": 10.475546836853027,
      "learning_rate": 3.633333333333334e-06,
      "loss": 0.4747,
      "step": 1910
    },
    {
      "epoch": 27.04225352112676,
      "grad_norm": 0.4949599802494049,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 0.4138,
      "step": 1920
    },
    {
      "epoch": 27.183098591549296,
      "grad_norm": 6.0303778648376465,
      "learning_rate": 3.566666666666667e-06,
      "loss": 0.237,
      "step": 1930
    },
    {
      "epoch": 27.323943661971832,
      "grad_norm": 14.668098449707031,
      "learning_rate": 3.5333333333333335e-06,
      "loss": 0.2577,
      "step": 1940
    },
    {
      "epoch": 27.464788732394368,
      "grad_norm": 19.399688720703125,
      "learning_rate": 3.5e-06,
      "loss": 0.8152,
      "step": 1950
    },
    {
      "epoch": 27.6056338028169,
      "grad_norm": 18.887327194213867,
      "learning_rate": 3.4666666666666672e-06,
      "loss": 0.5112,
      "step": 1960
    },
    {
      "epoch": 27.746478873239436,
      "grad_norm": 11.016018867492676,
      "learning_rate": 3.4333333333333336e-06,
      "loss": 0.352,
      "step": 1970
    },
    {
      "epoch": 27.887323943661972,
      "grad_norm": 17.162031173706055,
      "learning_rate": 3.4000000000000005e-06,
      "loss": 0.4102,
      "step": 1980
    },
    {
      "epoch": 28.028169014084508,
      "grad_norm": 0.7383953332901001,
      "learning_rate": 3.366666666666667e-06,
      "loss": 0.3789,
      "step": 1990
    },
    {
      "epoch": 28.169014084507044,
      "grad_norm": 15.48279857635498,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.2201,
      "step": 2000
    },
    {
      "epoch": 28.169014084507044,
      "eval_bleu-4": 0.25065635577157486,
      "eval_rouge-1": 60.11633,
      "eval_rouge-2": 27.462049999999998,
      "eval_rouge-l": 49.28076999999999,
      "eval_runtime": 40.7501,
      "eval_samples_per_second": 1.227,
      "eval_steps_per_second": 0.098,
      "step": 2000
    },
    {
      "epoch": 28.309859154929576,
      "grad_norm": 16.38001823425293,
      "learning_rate": 3.3000000000000006e-06,
      "loss": 0.4517,
      "step": 2010
    },
    {
      "epoch": 28.450704225352112,
      "grad_norm": 35.61602020263672,
      "learning_rate": 3.266666666666667e-06,
      "loss": 0.4623,
      "step": 2020
    },
    {
      "epoch": 28.591549295774648,
      "grad_norm": 9.569701194763184,
      "learning_rate": 3.2333333333333334e-06,
      "loss": 0.2697,
      "step": 2030
    },
    {
      "epoch": 28.732394366197184,
      "grad_norm": 10.704246520996094,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.3292,
      "step": 2040
    },
    {
      "epoch": 28.87323943661972,
      "grad_norm": 17.541149139404297,
      "learning_rate": 3.1666666666666667e-06,
      "loss": 0.4064,
      "step": 2050
    },
    {
      "epoch": 29.014084507042252,
      "grad_norm": 15.720829963684082,
      "learning_rate": 3.133333333333334e-06,
      "loss": 0.7294,
      "step": 2060
    },
    {
      "epoch": 29.154929577464788,
      "grad_norm": 10.13874626159668,
      "learning_rate": 3.1000000000000004e-06,
      "loss": 0.2188,
      "step": 2070
    },
    {
      "epoch": 29.295774647887324,
      "grad_norm": 11.13572883605957,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.2961,
      "step": 2080
    },
    {
      "epoch": 29.43661971830986,
      "grad_norm": 21.028703689575195,
      "learning_rate": 3.0333333333333337e-06,
      "loss": 0.4109,
      "step": 2090
    },
    {
      "epoch": 29.577464788732396,
      "grad_norm": 11.028450965881348,
      "learning_rate": 3e-06,
      "loss": 0.5277,
      "step": 2100
    },
    {
      "epoch": 29.718309859154928,
      "grad_norm": 10.935931205749512,
      "learning_rate": 2.9666666666666673e-06,
      "loss": 0.2577,
      "step": 2110
    },
    {
      "epoch": 29.859154929577464,
      "grad_norm": 16.954299926757812,
      "learning_rate": 2.9333333333333338e-06,
      "loss": 0.541,
      "step": 2120
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.5560674667358398,
      "learning_rate": 2.9e-06,
      "loss": 0.4479,
      "step": 2130
    },
    {
      "epoch": 30.140845070422536,
      "grad_norm": 17.438344955444336,
      "learning_rate": 2.866666666666667e-06,
      "loss": 0.5438,
      "step": 2140
    },
    {
      "epoch": 30.281690140845072,
      "grad_norm": 2.917478561401367,
      "learning_rate": 2.8333333333333335e-06,
      "loss": 0.2947,
      "step": 2150
    },
    {
      "epoch": 30.422535211267604,
      "grad_norm": 11.023880004882812,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.304,
      "step": 2160
    },
    {
      "epoch": 30.56338028169014,
      "grad_norm": 12.67345142364502,
      "learning_rate": 2.766666666666667e-06,
      "loss": 0.2654,
      "step": 2170
    },
    {
      "epoch": 30.704225352112676,
      "grad_norm": 10.77738094329834,
      "learning_rate": 2.7333333333333336e-06,
      "loss": 0.506,
      "step": 2180
    },
    {
      "epoch": 30.845070422535212,
      "grad_norm": 0.6422348618507385,
      "learning_rate": 2.7000000000000004e-06,
      "loss": 0.3079,
      "step": 2190
    },
    {
      "epoch": 30.985915492957748,
      "grad_norm": 17.991758346557617,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.332,
      "step": 2200
    },
    {
      "epoch": 31.12676056338028,
      "grad_norm": 17.47017478942871,
      "learning_rate": 2.6333333333333332e-06,
      "loss": 0.4016,
      "step": 2210
    },
    {
      "epoch": 31.267605633802816,
      "grad_norm": 9.416793823242188,
      "learning_rate": 2.6e-06,
      "loss": 0.3782,
      "step": 2220
    },
    {
      "epoch": 31.408450704225352,
      "grad_norm": 14.999724388122559,
      "learning_rate": 2.566666666666667e-06,
      "loss": 0.2616,
      "step": 2230
    },
    {
      "epoch": 31.549295774647888,
      "grad_norm": 14.804073333740234,
      "learning_rate": 2.5333333333333338e-06,
      "loss": 0.2949,
      "step": 2240
    },
    {
      "epoch": 31.690140845070424,
      "grad_norm": 16.85976219177246,
      "learning_rate": 2.5e-06,
      "loss": 0.4606,
      "step": 2250
    },
    {
      "epoch": 31.830985915492956,
      "grad_norm": 3.433089017868042,
      "learning_rate": 2.466666666666667e-06,
      "loss": 0.2215,
      "step": 2260
    },
    {
      "epoch": 31.971830985915492,
      "grad_norm": 12.168573379516602,
      "learning_rate": 2.4333333333333335e-06,
      "loss": 0.4893,
      "step": 2270
    },
    {
      "epoch": 32.11267605633803,
      "grad_norm": 6.501023292541504,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.2008,
      "step": 2280
    },
    {
      "epoch": 32.25352112676056,
      "grad_norm": 22.107954025268555,
      "learning_rate": 2.3666666666666667e-06,
      "loss": 0.3212,
      "step": 2290
    },
    {
      "epoch": 32.394366197183096,
      "grad_norm": 2.9003167152404785,
      "learning_rate": 2.3333333333333336e-06,
      "loss": 0.4799,
      "step": 2300
    },
    {
      "epoch": 32.53521126760563,
      "grad_norm": 12.976329803466797,
      "learning_rate": 2.3000000000000004e-06,
      "loss": 0.4928,
      "step": 2310
    },
    {
      "epoch": 32.67605633802817,
      "grad_norm": 16.050216674804688,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.22,
      "step": 2320
    },
    {
      "epoch": 32.816901408450704,
      "grad_norm": 5.903098106384277,
      "learning_rate": 2.2333333333333333e-06,
      "loss": 0.328,
      "step": 2330
    },
    {
      "epoch": 32.95774647887324,
      "grad_norm": 12.591045379638672,
      "learning_rate": 2.2e-06,
      "loss": 0.2542,
      "step": 2340
    },
    {
      "epoch": 33.098591549295776,
      "grad_norm": 14.167654991149902,
      "learning_rate": 2.166666666666667e-06,
      "loss": 0.3046,
      "step": 2350
    },
    {
      "epoch": 33.23943661971831,
      "grad_norm": 14.055261611938477,
      "learning_rate": 2.133333333333334e-06,
      "loss": 0.2081,
      "step": 2360
    },
    {
      "epoch": 33.38028169014085,
      "grad_norm": 0.7772217988967896,
      "learning_rate": 2.1000000000000002e-06,
      "loss": 0.2654,
      "step": 2370
    },
    {
      "epoch": 33.521126760563384,
      "grad_norm": 8.631399154663086,
      "learning_rate": 2.0666666666666666e-06,
      "loss": 0.3312,
      "step": 2380
    },
    {
      "epoch": 33.66197183098591,
      "grad_norm": 7.1670708656311035,
      "learning_rate": 2.0333333333333335e-06,
      "loss": 0.2379,
      "step": 2390
    },
    {
      "epoch": 33.80281690140845,
      "grad_norm": 4.2623090744018555,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.6358,
      "step": 2400
    },
    {
      "epoch": 33.943661971830984,
      "grad_norm": 0.37176600098609924,
      "learning_rate": 1.9666666666666668e-06,
      "loss": 0.342,
      "step": 2410
    },
    {
      "epoch": 34.08450704225352,
      "grad_norm": 5.812838077545166,
      "learning_rate": 1.9333333333333336e-06,
      "loss": 0.2962,
      "step": 2420
    },
    {
      "epoch": 34.225352112676056,
      "grad_norm": 14.690574645996094,
      "learning_rate": 1.9000000000000002e-06,
      "loss": 0.1729,
      "step": 2430
    },
    {
      "epoch": 34.36619718309859,
      "grad_norm": 14.384076118469238,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.5364,
      "step": 2440
    },
    {
      "epoch": 34.50704225352113,
      "grad_norm": 13.528654098510742,
      "learning_rate": 1.8333333333333333e-06,
      "loss": 0.4068,
      "step": 2450
    },
    {
      "epoch": 34.647887323943664,
      "grad_norm": 4.561429023742676,
      "learning_rate": 1.8000000000000001e-06,
      "loss": 0.2492,
      "step": 2460
    },
    {
      "epoch": 34.7887323943662,
      "grad_norm": 8.944214820861816,
      "learning_rate": 1.7666666666666668e-06,
      "loss": 0.2839,
      "step": 2470
    },
    {
      "epoch": 34.929577464788736,
      "grad_norm": 2.6601667404174805,
      "learning_rate": 1.7333333333333336e-06,
      "loss": 0.2921,
      "step": 2480
    },
    {
      "epoch": 35.070422535211264,
      "grad_norm": 21.616235733032227,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 0.2418,
      "step": 2490
    },
    {
      "epoch": 35.2112676056338,
      "grad_norm": 0.41497862339019775,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 0.3589,
      "step": 2500
    },
    {
      "epoch": 35.2112676056338,
      "eval_bleu-4": 0.2355097419908567,
      "eval_rouge-1": 55.46782000000001,
      "eval_rouge-2": 25.691,
      "eval_rouge-l": 46.47383,
      "eval_runtime": 42.0917,
      "eval_samples_per_second": 1.188,
      "eval_steps_per_second": 0.095,
      "step": 2500
    },
    {
      "epoch": 35.352112676056336,
      "grad_norm": 5.2651567459106445,
      "learning_rate": 1.6333333333333335e-06,
      "loss": 0.1411,
      "step": 2510
    },
    {
      "epoch": 35.49295774647887,
      "grad_norm": 17.855770111083984,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.3633,
      "step": 2520
    },
    {
      "epoch": 35.63380281690141,
      "grad_norm": 2.3362414836883545,
      "learning_rate": 1.566666666666667e-06,
      "loss": 0.3855,
      "step": 2530
    },
    {
      "epoch": 35.774647887323944,
      "grad_norm": 0.3220635652542114,
      "learning_rate": 1.5333333333333334e-06,
      "loss": 0.2739,
      "step": 2540
    },
    {
      "epoch": 35.91549295774648,
      "grad_norm": 8.375245094299316,
      "learning_rate": 1.5e-06,
      "loss": 0.3793,
      "step": 2550
    },
    {
      "epoch": 36.056338028169016,
      "grad_norm": 8.578117370605469,
      "learning_rate": 1.4666666666666669e-06,
      "loss": 0.2197,
      "step": 2560
    },
    {
      "epoch": 36.19718309859155,
      "grad_norm": 9.419294357299805,
      "learning_rate": 1.4333333333333335e-06,
      "loss": 0.1931,
      "step": 2570
    },
    {
      "epoch": 36.33802816901409,
      "grad_norm": 11.461423873901367,
      "learning_rate": 1.4000000000000001e-06,
      "loss": 0.414,
      "step": 2580
    },
    {
      "epoch": 36.478873239436616,
      "grad_norm": 0.34906595945358276,
      "learning_rate": 1.3666666666666668e-06,
      "loss": 0.3016,
      "step": 2590
    },
    {
      "epoch": 36.61971830985915,
      "grad_norm": 13.17208480834961,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.3049,
      "step": 2600
    },
    {
      "epoch": 36.76056338028169,
      "grad_norm": 4.231804370880127,
      "learning_rate": 1.3e-06,
      "loss": 0.2685,
      "step": 2610
    },
    {
      "epoch": 36.901408450704224,
      "grad_norm": 2.4862537384033203,
      "learning_rate": 1.2666666666666669e-06,
      "loss": 0.3027,
      "step": 2620
    },
    {
      "epoch": 37.04225352112676,
      "grad_norm": 13.18086051940918,
      "learning_rate": 1.2333333333333335e-06,
      "loss": 0.2738,
      "step": 2630
    },
    {
      "epoch": 37.183098591549296,
      "grad_norm": 15.516140937805176,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.2426,
      "step": 2640
    },
    {
      "epoch": 37.32394366197183,
      "grad_norm": 10.90026569366455,
      "learning_rate": 1.1666666666666668e-06,
      "loss": 0.1519,
      "step": 2650
    },
    {
      "epoch": 37.46478873239437,
      "grad_norm": 3.5382556915283203,
      "learning_rate": 1.1333333333333334e-06,
      "loss": 0.1713,
      "step": 2660
    },
    {
      "epoch": 37.605633802816904,
      "grad_norm": 10.001559257507324,
      "learning_rate": 1.1e-06,
      "loss": 0.6091,
      "step": 2670
    },
    {
      "epoch": 37.74647887323944,
      "grad_norm": 17.467918395996094,
      "learning_rate": 1.066666666666667e-06,
      "loss": 0.3385,
      "step": 2680
    },
    {
      "epoch": 37.88732394366197,
      "grad_norm": 5.539656639099121,
      "learning_rate": 1.0333333333333333e-06,
      "loss": 0.1761,
      "step": 2690
    },
    {
      "epoch": 38.028169014084504,
      "grad_norm": 9.080482482910156,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.3852,
      "step": 2700
    },
    {
      "epoch": 38.16901408450704,
      "grad_norm": 17.99774932861328,
      "learning_rate": 9.666666666666668e-07,
      "loss": 0.4724,
      "step": 2710
    },
    {
      "epoch": 38.309859154929576,
      "grad_norm": 13.089006423950195,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.364,
      "step": 2720
    },
    {
      "epoch": 38.45070422535211,
      "grad_norm": 17.903003692626953,
      "learning_rate": 9.000000000000001e-07,
      "loss": 0.1912,
      "step": 2730
    },
    {
      "epoch": 38.59154929577465,
      "grad_norm": 16.391881942749023,
      "learning_rate": 8.666666666666668e-07,
      "loss": 0.2162,
      "step": 2740
    },
    {
      "epoch": 38.732394366197184,
      "grad_norm": 8.283145904541016,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.1087,
      "step": 2750
    },
    {
      "epoch": 38.87323943661972,
      "grad_norm": 14.451016426086426,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.3592,
      "step": 2760
    },
    {
      "epoch": 39.014084507042256,
      "grad_norm": 8.179468154907227,
      "learning_rate": 7.666666666666667e-07,
      "loss": 0.2321,
      "step": 2770
    },
    {
      "epoch": 39.15492957746479,
      "grad_norm": 0.30783024430274963,
      "learning_rate": 7.333333333333334e-07,
      "loss": 0.3928,
      "step": 2780
    },
    {
      "epoch": 39.29577464788732,
      "grad_norm": 6.67086935043335,
      "learning_rate": 7.000000000000001e-07,
      "loss": 0.1384,
      "step": 2790
    },
    {
      "epoch": 39.436619718309856,
      "grad_norm": 14.243144035339355,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.2781,
      "step": 2800
    },
    {
      "epoch": 39.57746478873239,
      "grad_norm": 9.598101615905762,
      "learning_rate": 6.333333333333334e-07,
      "loss": 0.1674,
      "step": 2810
    },
    {
      "epoch": 39.71830985915493,
      "grad_norm": 25.43667984008789,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.5405,
      "step": 2820
    },
    {
      "epoch": 39.859154929577464,
      "grad_norm": 34.447025299072266,
      "learning_rate": 5.666666666666667e-07,
      "loss": 0.2142,
      "step": 2830
    },
    {
      "epoch": 40.0,
      "grad_norm": 13.183810234069824,
      "learning_rate": 5.333333333333335e-07,
      "loss": 0.2271,
      "step": 2840
    },
    {
      "epoch": 40.140845070422536,
      "grad_norm": 6.490387439727783,
      "learning_rate": 5.000000000000001e-07,
      "loss": 0.3401,
      "step": 2850
    },
    {
      "epoch": 40.28169014084507,
      "grad_norm": 6.971330165863037,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.2285,
      "step": 2860
    },
    {
      "epoch": 40.42253521126761,
      "grad_norm": 12.63440227508545,
      "learning_rate": 4.333333333333334e-07,
      "loss": 0.1487,
      "step": 2870
    },
    {
      "epoch": 40.563380281690144,
      "grad_norm": 8.834787368774414,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.482,
      "step": 2880
    },
    {
      "epoch": 40.70422535211267,
      "grad_norm": 14.864590644836426,
      "learning_rate": 3.666666666666667e-07,
      "loss": 0.2837,
      "step": 2890
    },
    {
      "epoch": 40.84507042253521,
      "grad_norm": 2.0440022945404053,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.1828,
      "step": 2900
    },
    {
      "epoch": 40.985915492957744,
      "grad_norm": 14.335216522216797,
      "learning_rate": 3.0000000000000004e-07,
      "loss": 0.2425,
      "step": 2910
    },
    {
      "epoch": 41.12676056338028,
      "grad_norm": 10.815258979797363,
      "learning_rate": 2.666666666666667e-07,
      "loss": 0.4111,
      "step": 2920
    },
    {
      "epoch": 41.267605633802816,
      "grad_norm": 0.3313678503036499,
      "learning_rate": 2.3333333333333336e-07,
      "loss": 0.4785,
      "step": 2930
    },
    {
      "epoch": 41.40845070422535,
      "grad_norm": 12.68744945526123,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.1514,
      "step": 2940
    },
    {
      "epoch": 41.54929577464789,
      "grad_norm": 9.021117210388184,
      "learning_rate": 1.6666666666666668e-07,
      "loss": 0.3065,
      "step": 2950
    },
    {
      "epoch": 41.690140845070424,
      "grad_norm": 17.41205596923828,
      "learning_rate": 1.3333333333333336e-07,
      "loss": 0.179,
      "step": 2960
    },
    {
      "epoch": 41.83098591549296,
      "grad_norm": 19.636505126953125,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 0.1885,
      "step": 2970
    },
    {
      "epoch": 41.971830985915496,
      "grad_norm": 21.866708755493164,
      "learning_rate": 6.666666666666668e-08,
      "loss": 0.2496,
      "step": 2980
    },
    {
      "epoch": 42.11267605633803,
      "grad_norm": 22.06735610961914,
      "learning_rate": 3.333333333333334e-08,
      "loss": 0.1692,
      "step": 2990
    },
    {
      "epoch": 42.25352112676056,
      "grad_norm": 0.4941214919090271,
      "learning_rate": 0.0,
      "loss": 0.3127,
      "step": 3000
    },
    {
      "epoch": 42.25352112676056,
      "eval_bleu-4": 0.23670984183625898,
      "eval_rouge-1": 55.894980000000004,
      "eval_rouge-2": 24.816999999999997,
      "eval_rouge-l": 46.38146,
      "eval_runtime": 40.8561,
      "eval_samples_per_second": 1.224,
      "eval_steps_per_second": 0.098,
      "step": 3000
    }
  ],
  "logging_steps": 10,
  "max_steps": 3000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 43,
  "save_steps": 500,
  "total_flos": 4753747338018816.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
